"""
GPT MESTRE AUTÃ”NOMO - Interface Chainlit v3.0 STREAMING
ğŸš€ Sistema com streaming em tempo real e construÃ§Ã£o palavra por palavra
"""

import chainlit as cl
import asyncio
from datetime import datetime
import uuid
import os
import sys
import time
import re

# Adicionar o diretÃ³rio ao path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Imports do sistema
try:
    from config import config
    from agents.carlos import criar_carlos_maestro
    from utils.logger import get_logger
    
    system_logger = get_logger("chainlit_streaming")
    
except ImportError as e:
    print(f"âŒ Erro ao importar mÃ³dulos: {e}")
    exit(1)

# ConfiguraÃ§Ãµes do Chainlit
cl.config.name = "ğŸ§  GPT Mestre AutÃ´nomo v5.0 - STREAMING"
cl.config.human_timeout = 300  # 5 minutos para operaÃ§Ãµes complexas

# Estado global
carlos_instance = None
current_status_msg = None
current_streaming_msg = None

async def stream_agent_activity(agent_name: str, activity: str, duration: float = 2.0):
    """Simula atividade do agente em tempo real"""
    emoji_map = {
        "Carlos": "ğŸ‘‘",
        "TaskBreaker": "ğŸ”¨", 
        "SupervisorAI": "ğŸ§ ",
        "AutoMaster": "ğŸ’¼",
        "PromptCrafter": "ğŸ¨",
        "OrÃ¡culo": "ğŸ”®",
        "DeepAgent": "ğŸŒ",
        "Reflexor": "ğŸ”"
    }
    
    emoji = emoji_map.get(agent_name, "ğŸ¤–")
    
    # Criar mensagem de atividade
    activity_msg = cl.Message(
        content=f"{emoji} **{agent_name}**: {activity}",
        author=f"{agent_name} Activity"
    )
    await activity_msg.send()
    
    # Simular duraÃ§Ã£o da atividade
    steps = ["âš¡ Inicializando...", "ğŸ”„ Processando...", "âœ… ConcluÃ­do!"]
    
    for i, step in enumerate(steps):
        await asyncio.sleep(duration / len(steps))
        activity_msg.content = f"{emoji} **{agent_name}**: {activity}\n{step}"
        await activity_msg.update()

async def stream_oraculo_assembly(num_suboraculos: int = 6):
    """Simula assembleia do OrÃ¡culo em tempo real"""
    assembly_msg = cl.Message(
        content="ğŸ”® **Assembleia DinÃ¢mica Iniciada**\nâ³ Convocando suborÃ¡culos...",
        author="OrÃ¡culo v9.0"
    )
    await assembly_msg.send()
    
    # Simular convocaÃ§Ã£o dos suborÃ¡culos
    specialists = ["Ã‰tico", "Viabilidade", "Criativo", "Paradoxo", "Copy", "Pricing"]
    
    for i, specialist in enumerate(specialists[:num_suboraculos]):
        await asyncio.sleep(1.5)
        assembly_msg.content += f"\nâœ… {specialist} conectado"
        await assembly_msg.update()
    
    # Simular deliberaÃ§Ã£o
    await asyncio.sleep(2)
    assembly_msg.content += "\n\nğŸ—³ï¸ **DeliberaÃ§Ã£o em Andamento**"
    await assembly_msg.update()
    
    # Simular votos chegando
    for i in range(num_suboraculos):
        await asyncio.sleep(3)
        assembly_msg.content += f"\nğŸ“Š Voto {i+1}/{num_suboraculos} recebido"
        await assembly_msg.update()
    
    # ConclusÃ£o
    await asyncio.sleep(1)
    assembly_msg.content += "\n\nâœ¨ **Assembleia ConcluÃ­da - Resultado Aprovado!**"
    await assembly_msg.update()

async def stream_response_generation(response_text: str):
    """Gera resposta palavra por palavra em tempo real"""
    global current_streaming_msg
    
    # Criar mensagem para streaming
    current_streaming_msg = cl.Message(
        content="",
        author="Carlos v5.0"
    )
    await current_streaming_msg.send()
    
    # Dividir texto em palavras
    words = response_text.split()
    current_text = ""
    
    # Stream palavra por palavra
    for i, word in enumerate(words):
        current_text += word + " "
        current_streaming_msg.content = current_text + "â–Œ"  # Cursor piscando
        await current_streaming_msg.update()
        
        # Pausa baseada no tamanho da palavra (mais realista)
        delay = max(0.05, min(0.3, len(word) * 0.02))
        await asyncio.sleep(delay)
        
        # Pausas extras em pontuaÃ§Ã£o
        if word.endswith(('.', '!', '?', ':')):
            await asyncio.sleep(0.5)
        elif word.endswith(','):
            await asyncio.sleep(0.2)
    
    # Remover cursor e finalizar
    current_streaming_msg.content = current_text.strip()
    await current_streaming_msg.update()

async def simulate_multi_token_processing(user_input: str, max_tokens: int = 4000):
    """Simula processamento multi-token para respostas grandes"""
    
    # Detectar se vai precisar de mÃºltiplos tokens
    estimated_tokens = len(user_input.split()) * 2  # Estimativa grosseira
    
    if estimated_tokens > max_tokens * 0.8:  # 80% do limite
        # Mostrar que vai dividir em chunks
        chunk_msg = cl.Message(
            content="ğŸ§  **Detectada requisiÃ§Ã£o complexa**\nâš¡ Dividindo em mÃºltiplos chunks para processamento otimizado...",
            author="Sistema Multi-Token"
        )
        await chunk_msg.send()
        
        # Simular divisÃ£o em chunks
        num_chunks = max(2, estimated_tokens // max_tokens + 1)
        
        for i in range(num_chunks):
            await asyncio.sleep(1)
            chunk_msg.content += f"\nğŸ“¦ Chunk {i+1}/{num_chunks} processado"
            await chunk_msg.update()
        
        chunk_msg.content += "\nâœ… **Todos os chunks processados - Compilando resposta final...**"
        await chunk_msg.update()

class StreamingCarlosWrapper:
    """Wrapper para interceptar e adicionar streaming ao Carlos"""
    
    def __init__(self, carlos_instance):
        self.carlos = carlos_instance
    
    async def processar_com_streaming(self, entrada: str, contexto: dict = None):
        """Processa com streaming visual completo"""
        
        # 1. AnÃ¡lise inicial
        await stream_agent_activity("Carlos", "Analisando comando recebido", 1.5)
        
        # 2. Detectar se precisa multi-token
        await simulate_multi_token_processing(entrada)
        
        # 3. TaskBreaker analisando
        await stream_agent_activity("TaskBreaker", "Avaliando complexidade da tarefa", 2.0)
        
        # 4. SupervisorAI classificando
        await stream_agent_activity("SupervisorAI", "Classificando tipo de resposta necessÃ¡ria", 1.8)
        
        # 5. Se for plano de carreira, mostrar agentes especÃ­ficos
        if "plano" in entrada.lower() and "carreira" in entrada.lower():
            await stream_agent_activity("AutoMaster", "Criando estratÃ©gia de carreira completa", 3.0)
            await stream_agent_activity("PromptCrafter", "Otimizando estrutura do plano", 2.5)
        
        # 6. SimulaÃ§Ã£o da Assembleia do OrÃ¡culo
        await stream_oraculo_assembly(6)
        
        # 7. Processamento real do Carlos
        response = self.carlos.processar(entrada, contexto or {})
        
        # 8. Auditoria final
        await stream_agent_activity("Reflexor", "Realizando auditoria de qualidade", 1.5)
        
        # 9. Stream da resposta palavra por palavra
        await asyncio.sleep(1)
        await stream_response_generation(response)
        
        return response

@cl.on_chat_start
async def start():
    """Inicializa o sistema com feedback streaming"""
    global carlos_instance
    
    session_id = str(uuid.uuid4())[:8]
    
    # Loading sequence animada
    loading_msg = cl.Message(content="ğŸš€ **Inicializando GPT Mestre AutÃ´nomo v5.0 Streaming...**")
    await loading_msg.send()
    
    steps = [
        "ğŸ”§ Inicializando nÃºcleo do sistema...",
        "ğŸ‘‘ Carregando Carlos v5.0 Maestro...",
        "ğŸ§  Ativando SupervisorAI v2.0...", 
        "ğŸ” Inicializando Reflexor v2.0...",
        "ğŸŒ Conectando DeepAgent v2.0...",
        "ğŸ”® Despertando OrÃ¡culo v9.0...",
        "ğŸ’¼ Ativando AutoMaster v2.0...",
        "ğŸ”¨ Preparando TaskBreaker v2.0...",
        "ğŸ§  Inicializando PsyMind v2.0...",
        "ğŸ¨ Carregando PromptCrafter v3.0...",
        "ğŸ§  Conectando memÃ³ria vetorial...",
        "âš¡ Configurando streaming em tempo real...",
        "âœ… Sistema 100% operacional!"
    ]
    
    for step in steps:
        await asyncio.sleep(0.4)
        loading_msg.content = f"ğŸš€ **GPT Mestre AutÃ´nomo v5.0 Streaming**\n\n{step}"
        await loading_msg.update()
    
    try:
        # Inicializar Carlos
        carlos_base = criar_carlos_maestro(
            supervisor_ativo=True,
            reflexor_ativo=True,
            deepagent_ativo=True,
            oraculo_ativo=True,
            automaster_ativo=True,
            taskbreaker_ativo=True,
            psymind_ativo=True,
            promptcrafter_ativo=True,
            memoria_ativa=True,
            modo_proativo=True
        )
        
        # Wrapper com streaming
        carlos_instance = StreamingCarlosWrapper(carlos_base)
        
        # Mensagem de boas-vindas
        welcome_msg = cl.Message(
            content=f"""
ğŸ‰ **GPT Mestre AutÃ´nomo v5.0 STREAMING PRONTO!**

ğŸ”¥ **Funcionalidades STREAMING:**
â€¢ ğŸ“¡ **Feedback em Tempo Real** - Veja cada agente trabalhando
â€¢ âš¡ **Resposta Palavra por Palavra** - ConstruÃ§Ã£o ao vivo  
â€¢ ğŸ§  **Assembleia DinÃ¢mica Visual** - Veja os suborÃ¡culos deliberando
â€¢ ğŸš€ **Sistema Multi-Token** - Quebra automÃ¡tica para respostas grandes
â€¢ ğŸª **TransparÃªncia Total** - Acompanhe todo o processo

ğŸ§  **Sistema Multi-Agente Ativo:**
â€¢ ğŸ‘‘ Carlos v5.0 - Maestro com Streaming
â€¢ ğŸ”® OrÃ¡culo v9.0 - Assembleia DinÃ¢mica Visual
â€¢ ğŸ’¼ AutoMaster v2.0 - Planejamento EstratÃ©gico
â€¢ ğŸ”¨ TaskBreaker v2.0 - AnÃ¡lise de Complexidade
â€¢ ğŸ” Reflexor v2.0 - Auditoria de Qualidade
â€¢ ğŸŒ DeepAgent v2.0 - Pesquisa Web em Tempo Real

ğŸ’¡ **Teste agora:**
"Crie um plano completo de carreira em programaÃ§Ã£o"

ğŸš€ **SessÃ£o ID**: {session_id}

**Oi Matheus! Como posso ajudÃ¡-lo hoje?** ğŸ˜Š
""",
            author="GPT Mestre AutÃ´nomo"
        )
        
        await welcome_msg.send()
        system_logger.info(f"ğŸš€ Sistema Streaming inicializado para sessÃ£o {session_id}")
        
    except Exception as e:
        error_msg = cl.Message(
            content=f"âŒ **Erro na inicializaÃ§Ã£o**: {str(e)}\n\nTente recarregar a pÃ¡gina."
        )
        await error_msg.send()
        system_logger.error(f"âŒ Erro na inicializaÃ§Ã£o: {e}")

@cl.on_message
async def main(message: cl.Message):
    """Processa mensagens com streaming completo"""
    global carlos_instance
    
    if not carlos_instance:
        error_msg = cl.Message(content="âŒ Sistema nÃ£o inicializado. Recarregue a pÃ¡gina.")
        await error_msg.send()
        return
    
    user_input = message.content.strip()
    
    try:
        # Processar com streaming visual completo
        await carlos_instance.processar_com_streaming(user_input, {})
        
        system_logger.info(f"âœ… Resposta streaming gerada para: {user_input[:50]}...")
        
    except Exception as e:
        error_msg = cl.Message(
            content=f"âŒ **Erro no processamento**: {str(e)}\n\nTente uma pergunta mais simples.",
            author="Sistema"
        )
        await error_msg.send()
        system_logger.error(f"âŒ Erro no processamento streaming: {e}")

@cl.on_stop
async def stop():
    """Limpa recursos quando o chat para"""
    global carlos_instance, current_status_msg, current_streaming_msg
    carlos_instance = None
    current_status_msg = None
    current_streaming_msg = None
    system_logger.info("ğŸ›‘ SessÃ£o streaming encerrada")

if __name__ == "__main__":
    cl.run()